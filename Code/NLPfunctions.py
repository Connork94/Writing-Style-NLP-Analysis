## get parts of speech

import nltk
from nltk.corpus import wordnet
from collections import Counter

# This function finds and returns the most likely part of speech for a particular word
def get_part_of_speech(word):
    probable_part_of_speech = wordnet.synsets(word)

    pos_counts = Counter()
    pos_counts["n"] = len([item for item in probable_part_of_speech if item.pos() == "n"])
    pos_counts["v"] = len([item for item in probable_part_of_speech if item.pos() == "v"])
    pos_counts["a"] = len([item for item in probable_part_of_speech if item.pos() == "a"])
    pos_counts["r"] = len([item for item in probable_part_of_speech if item.pos() == "r"])

    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]
    return most_likely_part_of_speech


## tokenize and lemmatize

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer


def tokenize_lemmatize_word(text):
    lemmatizer = WordNetLemmatizer()
    tokenized_string = word_tokenize(text)
    lemmatized_pos = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized_string]
    return lemmatized_pos


from chardet import detect


# finds and returns the type of encoding of a text file
def get_encoding_type(file):
    with open(file, 'rb') as f:
        rawdata = f.read()
    return detect(rawdata)['encoding']


import re


# function to process documents
def process_document(document, i, list_of_codes, stop_words):
    # load document and remove unwanted characters
    text = document.read()
    text = text.replace("\n", " ")
    text = text.replace("â€”", " ")
    text = text.replace(".", " ")
    text = text.lower()
    text = re.sub(r'_*', '', text)
    text = re.sub(r'-(?!\w)|(?<!\w)-', '', text)

    # get file name
    file_name = i
    text_name = file_name.replace(".txt", "")

    # get year
    text_year = re.match("[\d]{4}", text_name)
    text_year = text_year.group(0)
    # print(text_year)

    # remove year from title
    text_name = re.sub(r'\d*', '', text_name)
    # print(text_name)

    # Tokenize text
    # find number of words
    tokenized_text = word_tokenize(text)
    # print(tokenized_text)
    word_count = len(tokenized_text)
    total_corpus_word_count = word_count

    # create dictionary which will hold all info
    text_dict = {
        "text_name": text_name,
        "text_year": text_year,
        "text": text,
        "text_word_count": word_count
    }

    # Lemmatize words
    lemmatized_words = tokenize_lemmatize_word(text)
    # POS tag words
    pos_tags_new = nltk.pos_tag(lemmatized_words)

    word_list = []



    # Here we can change the types of words we are looking for...
    for token in pos_tags_new:
        if token[1] in list_of_codes:
            if token[0] not in stop_words:
                # print(token[0])
                word_list.append(token[0])
    # add common adjectives to dictionary
    text_dict["word_list"] = word_list

    # Create bag of words
    bag_of_words = Counter(word_list)
    bag_of_words = bag_of_words.most_common()
    bag_of_words_common = []
    for word in bag_of_words:
        if word[1] > 1:
            bag_of_words_common.append(word)

    # add common adjectives with counts to dictionary
    text_dict["common_word_list_count"] = bag_of_words_common

    # remove complete text. We do not need this data in the dict any more
    try:
        text_dict.pop("text")
    except:
        print('text already popped')
    return(text_dict)

import matplotlib as mpl
import matplotlib.pyplot as plt

# create a function to output matplotlib frequency chart of a specific word
def output_graph_combinedfreqprop(word, corpus, folder_images, word_percentage_dict, text_names_list, average_freq, word_type):
    # plot freq line
    x_axis = []
    y_axis = []
    list1 = []
    for i in range(len(corpus)):
        name = (corpus[i]['text_year'] + " " + corpus[i]['text_name'])
        x_axis.append(name)
        if word in corpus[i]['word_list']:
            items = corpus[i]['word_list']
            # print(items)
            count = items.count(word)
            # print(count)
            list1.append(count)
        else:
            list1.append(0)
        # print(len(list))
        y_axis = list1

    # create vars for percentage line
    y_axis2 = []
    list2 = []
    ref_line = []
    for i in range(len(word_percentage_dict['Words'])):
        if word_percentage_dict['Words'][i] == word:
            for x in text_names_list:
                y_axis2.append((word_percentage_dict[x][i] * 100))
                ref_line.append((average_freq[i] * 100))

    fig, ax1 = plt.subplots()
    # f = plt.figure()
    fig.set_figwidth(30)
    fig.set_figheight(5)

    ax1.set_xlabel('Texts')
    ax1.set_ylabel('Frequency', color='royalblue')
    plot_1 = ax1.plot(x_axis, y_axis, marker='o', color='royalblue', label="Absolute Frequency")
    ax1.tick_params(axis='y', labelcolor='royalblue')
    # ax1.xlim([-2,len(x_axis)+2])
    # ax1.xticks(rotation=90)

    # plt.plot(x_axis, y_axis, marker='o', label = "Frequency")
    plt.xlim([-2, len(x_axis) + 2])
    plt.xticks(rotation=90)

    # change font sizes
    plt.rc('axes', labelsize=15)
    plt.rc('axes', titlesize=20)

    ax2 = ax1.twinx()

    ax2.set_ylabel('Percentage %', color='orangered')
    plot_2 = ax2.plot(x_axis, y_axis2, marker='x', color='orangered', label="Relative Frequency")
    ax2.tick_params(axis='y', labelcolor='orangered')

    # plot percentage line
    # plt.plot(x_axis, y_axis2, marker='x')
    # plt.plot(x_axis, ref_line, 'k-', lw=1,dashes=[2, 2], label = "Average Percentage")
    plot_3 = ax2.plot(x_axis, ref_line, 'k-', lw=1, dashes=[2, 2], label="Relative Frequency for Entire Corpus")
    title = "Usage of the word " + "'" + word + "' in each story over time"
    plt.title(title)

    lns = plot_1 + plot_2 + plot_3
    labels = [l.get_label() for l in lns]
    plt.legend(lns, labels, loc=0)

    # plt.show()
    file_name = folder_images + word_type + "_" + word + "_frequency_percentage" + ".png"
    plt.savefig(file_name, bbox_inches='tight', dpi=100)
    print(word + " frequency chart exported")
    plt.close()

def synonym_analysis(df, folder_dataset, word_type):
    words_list = df["Words"]

    synonyms_dict = {}

    for word in words_list:
        list_synonyms = []
        for syn in wordnet.synsets(word):
            for lemm in syn.lemmas():
                if lemm.name() not in list_synonyms:
                    list_synonyms.append(lemm.name())
        synonyms_dict[word] = list_synonyms

    synonyms_scores_dict = {}
    count = 0
    print("creating synonym dict")
    for word in synonyms_dict:
        # print(word)
        count += 1
        synonyms_scores_dict[word] = []
        for synonym in synonyms_dict[word]:
            temp_dict_word_score = {}
            for i in range(len(df)):
                if df["Words"][i] == synonym:
                    temp_dict_word_score[synonym] = df["total_weighted_score"][i]
                    # print(temp_dict_word_score)
                    synonyms_scores_dict[word].append(temp_dict_word_score)
        if count == [100, 500, 1000]:
            print(count)
        else:
            pass

    synonyms_scores_dict_scores_only = {}

    for word in synonyms_dict:
        # print(word)
        score_list_for_calculation = []
        for synonym in synonyms_dict[word]:
            for i in range(len(df)):
                if df["Words"][i] == synonym:
                    score = df["total_weighted_score"][i]
                    # print(temp_dict_word_score)
                    score_list_for_calculation.append(score)
        if len(score_list_for_calculation) != 0:
            max_num = np.max(score_list_for_calculation)
        else:
            max_num = 0
        if len(score_list_for_calculation) != 0:
            min_num = np.min(score_list_for_calculation)
        else:
            max_num = 0
        synonyms_scores_dict_scores_only[word] = np.linalg.norm(max_num - min_num)

    words = []
    differences = []
    synonyms_list = []

    for word in synonyms_scores_dict_scores_only:
        words.append(word)
        differences.append(synonyms_scores_dict_scores_only[word])
        synonyms_list.append(synonyms_scores_dict[word])

    df_prep = {}

    df_prep["words"] = words
    df_prep["differences"] = differences
    df_prep["synonyms"] = synonyms_list

    #df.to_excel(r'Words_and_differences.xlsx', index=False)

    file_name = folder_dataset + word_type + 's' + '_' + 'words_and_differences_' + datetime.now().strftime(
        "%Y%m%d-%H%M%S") + '.xlsx'
    df.to_excel(file_name, sheet_name='sheet1', index=False)
    print(file_name)
